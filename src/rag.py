import json
import os
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM

# D√©sactiver le parall√©lisme des tokenizers
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"

# Charger les donn√©es et pr√©parer les embeddings
@st.cache_resource
def load_and_prepare_data(file_path, model_name="all-MiniLM-L6-v2"):
    with open(file_path, 'r', encoding='utf-8') as file:
        product_data = [json.loads(line) for line in file]
    
    descriptions = [product.get("description", "") for product in product_data if "description" in product]
    
    # Segmentation des textes
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
    segmented_texts = []
    for description in descriptions:
        if isinstance(description, list):
            description = " ".join(description)
        if description:
            chunks = text_splitter.split_text(description)
            segmented_texts.extend(chunks)
    
    # G√©n√©rer des embeddings
    embedding_model = HuggingFaceEmbeddings(model_name=model_name)
    embeddings = embedding_model.embed_documents(segmented_texts)
    
    # Cr√©er un index FAISS
    dimension = len(embeddings[0])
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    
    # Cr√©er un docstore
    documents = [Document(page_content=text) for text in segmented_texts]
    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})
    index_to_docstore_id = {i: str(i) for i in range(len(documents))}
    
    # Cr√©er un vector store
    vector_store = FAISS(
        embedding_function=embedding_model,
        index=index,
        docstore=docstore,
        index_to_docstore_id=index_to_docstore_id
    )
    
    return vector_store

# Charger un mod√®le LLM (comme BLOOM) pour g√©n√©rer des r√©ponses
@st.cache_resource
def load_llm_model(model_name="bigscience/bloomz-560m"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return tokenizer, model

# Extraire et v√©rifier uniquement les informations pertinentes
def get_flexible_answer(query, retriever, max_results=3):
    # R√©cup√©rer les documents pertinents
    results = retriever.get_relevant_documents(query)
    
    if not results:
        return "Je ne sais pas."  # Aucun document trouv√©

    # Filtrage des documents pour s'assurer qu'ils sont pertinents
    matched_contexts = []
    query_words = set(query.lower().split())
    for result in results[:max_results]:
        document_words = set(result.page_content.lower().split())
        if query_words & document_words:  # V√©rifie s'il y a des mots en commun
            matched_contexts.append(result.page_content)

    if not matched_contexts:
        return "Je ne sais pas."  # Aucun document ne correspond directement

    # Retourner les informations pertinentes trouv√©es
    return "\n".join(matched_contexts).strip()

# G√©n√©rer une r√©ponse concise avec LLM
def generate_response(query, context, tokenizer, model, temperature, top_p):
    # R√©duction du contexte pour √©viter des prompts trop longs
    truncated_context = context[:1000]  # Limite √† 1000 caract√®res

    # Formulation d'un prompt clair et naturel
    prompt = f"""
    Vous √™tes un assistant qui r√©pond aux questions sur des produits en utilisant les informations suivantes :
    {truncated_context}

    R√©pondez directement √† la question suivante d'une mani√®re naturelle et claire :
    {query}

    Si aucune r√©ponse ne peut √™tre trouv√©e dans ces informations, r√©pondez : "Je ne sais pas."
    """
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,  # Limite la longueur de la r√©ponse g√©n√©r√©e
        temperature=temperature,
        top_p=top_p,
        num_return_sequences=1
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    #response = clean_response(response, query)
    #print(f"R√©ponse brute g√©n√©r√©e : {response}")
    #print(f"Prompt utilis√© : {prompt}")

    return response.strip()


#def clean_response(response, query):
   # """
    #Supprime uniquement les parties non d√©sir√©es tout en pr√©servant la r√©ponse g√©n√©r√©e.
   # """
    # Supprimer les instructions initiales si pr√©sentes
    #if "R√©pondez directement" in response:
        #response = response.split("R√©pondez directement", maxsplit=1)[-1]
    
    # Retirer les instructions r√©siduelles
   # if "Si aucune r√©ponse ne peut √™tre trouv√©e" in response:
        #response = response.split("Si aucune r√©ponse ne peut √™tre trouv√©e")[0]

    # Nettoyage final
   # return response.strip()


# Interface utilisateur avec Streamlit
def main():
    st.set_page_config(page_title="Descriptions Produits Amazon", layout="wide")
    st.title("üîçDescriptions De Produits Amazon")
    st.write("Posez une question pour obtenir des r√©ponses bas√©es sur l'ensemble de documents fournies.")
    
    file_path = 'data/meta.jsonl'  # Remplacez par le chemin r√©el de votre fichier JSON
    vector_store = load_and_prepare_data(file_path)
    retriever = vector_store.as_retriever()

    tokenizer, model = load_llm_model()

    # Barre lat√©rale pour les param√®tres
    with st.sidebar:
        st.header("üîß Param√®tres")
        temperature = st.slider("Temp√©rature (cr√©ativit√©)", 0.1, 2.0, 1.0, step=0.1)
        top_p = st.slider("Top-p (probabilit√© cumulative)", 0.1, 1.0, 0.9, step=0.1)
    
    # Entr√©e utilisateur
    query = st.text_input("üí¨ Posez Votre question :")

    if st.button("üöÄ Obtenir une r√©ponse"):
        if query.strip():
            # Obtenir les informations pertinentes
            context = get_flexible_answer(query, retriever)
            
            if context == "Je ne sais pas.":
                st.write("### R√©ponse :")
                st.write(context)
            else:
                # G√©n√©rer une r√©ponse concise avec les param√®tres choisis
                response = generate_response(query, context, tokenizer, model, temperature, top_p)
                st.write("### R√©ponse :")
                st.write(response)
                # Documenter les param√®tres et r√©sultats
                st.write("#### Param√®tres utilis√©s :")
                st.write(f"Temp√©rature : {temperature}, Top-p : {top_p}")
        else:
            st.write("Posez votre r√©ponse.")

if __name__ == "__main__":
    main()
